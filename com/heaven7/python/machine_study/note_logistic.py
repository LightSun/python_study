# coding:utf-8

# logistic 回归. 可以堪称概率估计

"""
logistic 回归.优缺点.
    优点： 计算代价不高，易于理解和实现。
    缺点：容易欠拟合， 分类精度可能不高.
    适用数据类型： 数值型，标称型.

ps:
    标称型：标称型目标变量的结果只在有限目标集中取值，如真与假(标称型目标变量主要用于分类)
    数值型：数值型目标变量则可以从无限的数值集合中取值，如0.100，42.001等 (数值型目标变量主要用于回归分析)
"""

"""
  sigmoid函数.
     海维塞德阶跃函数/单阶跃函数. 特点： 该函数在跳跃点上从0瞬间跳跃到1， 瞬间跳跃有很很难处理. 
     另外一个函数也有类似的性质。且更易处理。它就是sigmoid函数,  f(x) = 1/(1 + e^-x).
     x = 0, f(x) = 0。5
     x增大， f(x)接近于1
     x 减小，f(x)趋近于0.
     每个特征都乘以回归系数， 然后把所有值相加，将总和带入sigmoid函数。 得到0-1的一个值.任何>0.5的归为1类。 <0.5的归为0类.
     所以现在的问题就是  求 最佳回归系数？
"""

"""
梯度上升法: 要找到某函数的最大值， 最好的方法是沿着该函数的梯度方向探寻。
梯度下降法: 要找到某函数的最小值， 最好的方法是沿着该函数的梯度方向探寻。
============= 梯度上升法伪代码 (拟合logistic回归模型的最佳参数) ==================
每个回归系数 初始化为1
重复R次
   计算整个数据集的梯度
   使用alpha * gradient更新回归系数的向量
   返回回归系数  
"""

"""
随机梯度上升法
所有回归系数初始值为1
对数据集中的每个样本
     计算样本的梯度    
    使用alpha*gradient 更新回归系数值
return 回归系数值         
"""
